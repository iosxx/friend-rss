#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‹é“¾RSSè®¢é˜…èšåˆç¨‹åº
ä»å‹é“¾é¡µé¢å’Œæ‰‹åŠ¨é…ç½®åˆ—è¡¨ä¸­è·å–RSSæºï¼Œèšåˆæˆå¯é…ç½®çš„JSONæ–‡ä»¶
"""

import os
import json
import re
import logging
from datetime import datetime, timedelta, timezone
from email.utils import parsedate_to_datetime
from typing import List, Dict, Any, Tuple, Optional
from urllib.parse import urljoin, urlparse
import hashlib
from time import sleep
from concurrent.futures import ThreadPoolExecutor, as_completed

import requests
import yaml
from bs4 import BeautifulSoup
import feedparser
from requests.adapters import HTTPAdapter
from urllib3.util import Retry
import urllib3

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# åˆå§‹åŒ–loggerï¼ˆç¨åä¼šæ ¹æ®é…ç½®è®¾ç½®çº§åˆ«ï¼‰
logger = logging.getLogger(__name__)

# å®šä¹‰åŒ—äº¬æ—¶é—´æ—¶åŒº (UTC+8)
BEIJING_TZ = timezone(timedelta(hours=8))

# é»˜è®¤é…ç½®ï¼ˆå¯è¢« setting.yaml è¦†ç›–ï¼‰
DEFAULT_CONFIG = {
    'REQUEST_TIMEOUT': 10,
    'FEED_CHECK_TIMEOUT': 5,
    'REQUEST_RETRIES': 1,
    'RETRY_BACKOFF': 0.3,
    'MAX_WORKERS': 0,  # 0 è¡¨ç¤ºä¸ä½¿ç”¨å¹¶å‘
    'LOG_LEVEL': 'INFO',
    'CACHE_FILE': 'feed_cache.json',
    'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

def get_beijing_time():
    """è·å–å½“å‰åŒ—äº¬æ—¶é—´"""
    return datetime.now(BEIJING_TZ)

def parse_feed_time(time_tuple, timezone_correction: bool = True, original_time_str: Optional[str] = None):
    """è§£æfeedæ—¶é—´
    
    Args:
        time_tuple: feedparserè§£æçš„æ—¶é—´å…ƒç»„ï¼ˆé€šå¸¸ä¸ºUTCï¼‰
        timezone_correction: æ˜¯å¦è¿›è¡Œæ—¶åŒºæ ¡æ­£ï¼ˆTrue: è½¬ä¸ºåŒ—äº¬æ—¶é—´ï¼›False: ä¿ç•™å¯¹æ–¹æ–‡ç« çš„â€œå¢™ä¸Šæ—¶é—´â€å¹¶æ ‡æ³¨ä¸ºåŒ—äº¬æ—¶é—´ï¼‰
        original_time_str: åŸå§‹æ—¶é—´å­—ç¬¦ä¸²ï¼ˆå¦‚ RFC822 çš„ pubDateï¼‰ï¼Œç”¨äºåœ¨å…³é—­æ ¡æ­£æ—¶å‡†ç¡®ä¿ç•™å¢™ä¸Šæ—¶é—´
    Returns:
        datetime: å¸¦æ—¶åŒºä¿¡æ¯çš„æ—¶é—´
    """
    # å…³é—­æ ¡æ­£ï¼šå°½é‡ä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²æ¥ä¿ç•™â€œå¢™ä¸Šæ—¶é—´â€
    if not timezone_correction and original_time_str:
        try:
            dt = parsedate_to_datetime(original_time_str)
            # ä¿ç•™å¯¹æ–¹æ–‡ç« çš„å¢™ä¸Šæ—¶é—´ï¼ˆå‡ ç‚¹å°±æ˜¯å‡ ç‚¹ï¼‰ï¼Œä½†æ ‡æ³¨ä¸ºåŒ—äº¬æ—¶é—´
            # æ— è®ºåŸæœ¬å±äºå“ªä¸ªæ—¶åŒºï¼Œéƒ½åªå–å‡ºæ—¶åˆ†ç§’ä¸æ—¥æœŸï¼Œä¸åšæ¢ç®—
            local_dt = datetime(dt.year, dt.month, dt.day, dt.hour, dt.minute, dt.second, tzinfo=BEIJING_TZ)
            return local_dt
        except Exception as e:
            logger.debug(f"è§£æåŸå§‹æ—¶é—´å­—ç¬¦ä¸²å¤±è´¥ï¼Œå›é€€åˆ°å…ƒç»„å¤„ç†: {e}")
            # ç»§ç»­èµ°ä¸‹é¢çš„ time_tuple é€»è¾‘
    
    if not time_tuple:
        # æ²¡æœ‰ä»»ä½•æ—¶é—´å¯ç”¨ï¼Œä½¿ç”¨å½“å‰åŒ—äº¬æ—¶é—´
        return get_beijing_time()
    
    try:
        # feedparser çš„æ—¶é—´å…ƒç»„é€šå¸¸æ˜¯æŒ‰ UTC æä¾›
        utc_dt = datetime(*time_tuple[:6], tzinfo=timezone.utc)
        if timezone_correction:
            # å¼€å¯æ ¡æ­£ï¼šå°† UTC è½¬ä¸ºåŒ—äº¬æ—¶é—´
            return utc_dt.astimezone(BEIJING_TZ)
        else:
            # å…³é—­æ ¡æ­£ï¼šä¿ç•™å¢™ä¸Šæ—¶é—´â€”â€”ç”¨ UTC çš„æ—¶åˆ†ç§’ç›´æ¥æ ‡æ³¨ä¸ºåŒ—äº¬æ—¶é—´
            # æ³¨æ„ï¼šå½“ç¼ºå¤±åŸå§‹å­—ç¬¦ä¸²æ—¶ï¼Œæ— æ³•è¿˜åŸåŸæ—¶åŒºçš„å¢™ä¸Šæ—¶é—´ï¼Œåªèƒ½ä½¿ç”¨UTCå¢™ä¸Šæ—¶é—´
            wall_dt = datetime(utc_dt.year, utc_dt.month, utc_dt.day, utc_dt.hour, utc_dt.minute, utc_dt.second, tzinfo=BEIJING_TZ)
            return wall_dt
    except Exception as e:
        logger.warning(f"æ—¶é—´è§£æå¤±è´¥: {e}, ä½¿ç”¨å½“å‰æ—¶é—´ä»£æ›¿")
        return get_beijing_time()


class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨ï¼Œå­˜å‚¨å·²å‘ç°çš„RSSæºå’Œæ–‡ç« ID"""
    
    def __init__(self, cache_file: str = 'feed_cache.json'):
        self.cache_file = cache_file
        self.cache = self._load_cache()
    
    def _load_cache(self) -> dict:
        """åŠ è½½ç¼“å­˜"""
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                    # ç§»é™¤æ—§ç‰ˆæœ¬å¯èƒ½å­˜åœ¨çš„ article_ids
                    cache_data.pop('article_ids', None)
                    return cache_data
            except Exception as e:
                logger.warning(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
                return self._init_cache()
        return self._init_cache()
    
    def _init_cache(self) -> dict:
        """åˆå§‹åŒ–ç¼“å­˜ç»“æ„"""
        return {
            'feed_urls': {},  # {site_url: feed_url}
            'last_update': None
        }
    
    def save(self):
        """ä¿å­˜ç¼“å­˜"""
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache, f, ensure_ascii=False, indent=2)
            logger.debug(f"ç¼“å­˜å·²ä¿å­˜")
        except Exception as e:
            logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    def get_cached_feed_url(self, site_url: str) -> Optional[str]:
        """è·å–ç¼“å­˜çš„Feed URL"""
        return self.cache.get('feed_urls', {}).get(site_url)
    
    def set_feed_url(self, site_url: str, feed_url: str):
        """ç¼“å­˜Feed URL"""
        if 'feed_urls' not in self.cache:
            self.cache['feed_urls'] = {}
        self.cache['feed_urls'][site_url] = feed_url


class ConfigParser:
    """è§£æsetting.yamlé…ç½®æ–‡ä»¶"""
    
    def __init__(self, config_path: str = 'setting.yaml'):
        self.config_path = config_path
        self.config = self._load_config()
        self._setup_logging()
    
    def _load_config(self) -> dict:
        """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
        with open(self.config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def _setup_logging(self):
        """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_level = self.get_log_level()
        logging.basicConfig(
            level=getattr(logging, log_level),
            format='%(asctime)s - %(levelname)s - %(message)s',
            force=True  # å¼ºåˆ¶é‡æ–°é…ç½®
        )
    
    def get_link_pages(self) -> List[str]:
        """è·å–éœ€è¦çˆ¬å–çš„å‹é“¾é¡µé¢URLåˆ—è¡¨"""
        links = []
        for item in self.config.get('LINK', []):
            if isinstance(item, dict) and 'link' in item:
                links.append(item['link'])
        return links
    
    def get_link_page_rules(self) -> dict:
        """è·å–CSSé€‰æ‹©å™¨è§„åˆ™"""
        return self.config.get('link_page_rules', {})
    
    def get_block_sites(self) -> List[str]:
        """è·å–å±è”½ç«™ç‚¹åˆ—è¡¨"""
        return self.config.get('BLOCK_SITE', [])
    
    def get_block_site_reverse(self) -> bool:
        """è·å–æ˜¯å¦ä½¿ç”¨ç™½åå•æ¨¡å¼"""
        return self.config.get('BLOCK_SITE_REVERSE', False)
    
    def get_manual_links(self) -> List[Dict[str, str]]:
        """è·å–æ‰‹åŠ¨æ·»åŠ çš„å‹é“¾åˆ—è¡¨"""
        manual_links = []
        links_list = self.config.get('SETTINGS_FRIENDS_LINKS', {}).get('list', [])
        
        for item in links_list:
            if isinstance(item, list) and len(item) >= 3:
                link_dict = {
                    'name': item[0],
                    'url': item[1],
                    'avatar': item[2],
                    'feed_suffix': item[3] if len(item) > 3 else None
                }
                manual_links.append(link_dict)
        return manual_links
    
    def get_feed_suffixes(self) -> List[str]:
        """è·å–Feedåç¼€åˆ—è¡¨"""
        return self.config.get('feed_suffix', [])
    
    def get_max_posts(self) -> int:
        """è·å–æ¯ä¸ªç«™ç‚¹æœ€å¤šæŠ“å–æ–‡ç« æ•°"""
        return self.config.get('MAX_POSTS_NUM', 5)
    
    def get_outdate_days(self) -> int:
        """è·å–è¿‡æœŸæ–‡ç« å¤©æ•°
        
        Returns:
            int: è¿‡æœŸå¤©æ•°ï¼Œ0æˆ–è´Ÿæ•°è¡¨ç¤ºä¸é™åˆ¶
        """
        return self.config.get('OUTDATE_CLEAN', 180)

    def get_timezone_correction(self) -> bool:
        """è·å–æ˜¯å¦å¼€å¯æ—¶åŒºæ ¡æ­£
        
        Returns:
            bool: True - å°†æ‰€æœ‰æ—¶é—´æ¢ç®—ä¸ºåŒ—äº¬æ—¶é—´
                  False - ä¸æ¢ç®—ï¼Œä¿ç•™å¯¹æ–¹æ–‡ç« çš„å¢™ä¸Šæ—¶é—´ï¼Œä»…ä»¥åŒ—äº¬æ—¶é—´æ ‡æ³¨
        """
        return self.config.get('TIMEZONE_CORRECTION', True)

    def get_sort_by(self) -> str:
        """è·å–æ–‡ç« æ’åºæ–¹å¼
        
        Returns:
            str: 'pub_date' - æŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼ˆé»˜è®¤ï¼‰
                 'updated_at' - æŒ‰æ›´æ–°æ—¶é—´æ’åº
        """
        sort_by = self.config.get('SORT_BY', 'pub_date')
        if sort_by not in ['pub_date', 'updated_at']:
            logger.warning(f"æ— æ•ˆçš„æ’åºæ–¹å¼: {sort_by}ï¼Œä½¿ç”¨é»˜è®¤å€¼ pub_date")
            return 'pub_date'
        return sort_by

    def get_output_filename(self) -> str:
        """è·å–è¾“å‡ºJSONæ–‡ä»¶å
        
        Returns:
            str: è¾“å‡ºæ–‡ä»¶åï¼ˆç›¸å¯¹ä»“åº“æ ¹ç›®å½•ï¼‰ï¼Œé»˜è®¤ 'data.json'
        """
        return self.config.get('OUTPUT_JSON_FILENAME', 'data.json')
    
    def get_log_level(self) -> str:
        """è·å–æ—¥å¿—çº§åˆ«"""
        return self.config.get('LOG_LEVEL', 'INFO').upper()
    
    def get_max_workers(self) -> int:
        """è·å–å¹¶å‘å¤„ç†çº¿ç¨‹æ•°"""
        return self.config.get('MAX_WORKERS', 0)
    
    def get_request_timeout(self) -> int:
        """è·å–HTTPè¯·æ±‚è¶…æ—¶æ—¶é—´"""
        return self.config.get('REQUEST_TIMEOUT', 10)
    
    def get_feed_check_timeout(self) -> int:
        """è·å–Feed URLæ£€æŸ¥è¶…æ—¶æ—¶é—´"""
        return self.config.get('FEED_CHECK_TIMEOUT', 5)
    
    def get_request_retries(self) -> int:
        """è·å–HTTPè¯·æ±‚é‡è¯•æ¬¡æ•°"""
        return self.config.get('REQUEST_RETRIES', 1)
    
    def get_retry_backoff(self) -> float:
        """è·å–é‡è¯•é€€é¿ç³»æ•°"""
        return self.config.get('RETRY_BACKOFF', 0.3)
    
    def get_cache_file(self) -> str:
        """è·å–ç¼“å­˜æ–‡ä»¶å"""
        return self.config.get('CACHE_FILE', 'feed_cache.json')
    
    def get_user_agent(self) -> str:
        """è·å–User-Agent"""
        return self.config.get('USER_AGENT', DEFAULT_CONFIG['USER_AGENT'])


class SiteFilter:
    """ç«™ç‚¹è¿‡æ»¤å™¨ï¼Œå¤„ç†é»‘/ç™½åå•"""
    
    def __init__(self, block_sites: List[str], reverse: bool = False):
        self.block_sites = block_sites
        self.reverse = reverse
    
    def is_blocked(self, url: str) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦è¢«å±è”½
        
        é»‘åå•æ¨¡å¼ (reverse=False): åŒ¹é…çš„è¢«å±è”½ï¼Œå…¶ä»–å…è®¸
        ç™½åå•æ¨¡å¼ (reverse=True): åŒ¹é…çš„è¢«å…è®¸ï¼Œå…¶ä»–å±è”½
        """
        for pattern in self.block_sites:
            if re.search(pattern, url):
                # åŒ¹é…åˆ°è§„åˆ™
                # é»‘åå•æ¨¡å¼: åŒ¹é…çš„è¢«å±è”½
                if not self.reverse:
                    return True
                # ç™½åå•æ¨¡å¼: åŒ¹é…çš„è¢«å…è®¸
                else:
                    return False
        
        # æœªåŒ¹é…åˆ°è§„åˆ™
        # é»‘åå•æ¨¡å¼: æœªåŒ¹é…çš„å…è®¸
        if not self.reverse:
            return False
        # ç™½åå•æ¨¡å¼: æœªåŒ¹é…çš„å±è”½
        else:
            return True


class LinkPageScraper:
    """å‹é“¾é¡µé¢çˆ¬è™«"""
    
    def __init__(self, rules: dict, request_timeout: int = 10, user_agent: str = None):
        self.rules = rules
        self.request_timeout = request_timeout
        self.user_agent = user_agent or DEFAULT_CONFIG['USER_AGENT']
        self.session = self._create_session()
    
    def _create_session(self) -> requests.Session:
        """åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„requestsä¼šè¯"""
        session = requests.Session()
        session.headers.update({'User-Agent': self.user_agent})
        session.verify = False
        return session
    
    def scrape(self, url: str) -> List[Dict[str, str]]:
        """ä»å‹é“¾é¡µé¢çˆ¬å–é“¾æ¥"""
        try:
            logger.info(f"æ­£åœ¨çˆ¬å–å‹é“¾é¡µé¢: {url}")
            response = self.session.get(url, timeout=self.request_timeout)
            response.encoding = 'utf-8'
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            links = []
            author_elements = soup.select(self.rules.get('author', [{}])[0].get('selector', ''))
            
            for author_elem in author_elements:
                try:
                    # æŸ¥æ‰¾è¯¥ä½œè€…å…ƒç´ å¯¹åº”çš„é“¾æ¥
                    link_elem = author_elem.find_parent().find('a') if author_elem.find_parent() else author_elem
                    if not link_elem:
                        link_elem = author_elem
                    
                    link_url = link_elem.get('href') or link_elem.get('data-href', '')
                    author_name = author_elem.get_text(strip=True) or link_elem.get_text(strip=True)
                    
                    # å°è¯•è·å–å¤´åƒ
                    avatar = ''
                    img_elem = author_elem.find_parent().find('img') if author_elem.find_parent() else None
                    if not img_elem:
                        img_elem = author_elem.find('img')
                    if img_elem:
                        avatar = img_elem.get('src', '')
                    
                    if link_url and author_name:
                        # è§„èŒƒåŒ–URL
                        if not link_url.startswith('http'):
                            link_url = urljoin(url, link_url)
                        
                        links.append({
                            'name': author_name,
                            'url': link_url,
                            'avatar': avatar
                        })
                except Exception as e:
                    logger.debug(f"çˆ¬å–å•æ¡é“¾æ¥å¤±è´¥: {e}")
                    continue
            
            logger.info(f"ä»{url}æˆåŠŸçˆ¬å–{len(links)}æ¡é“¾æ¥")
            return links
        except requests.Timeout:
            logger.error(f"çˆ¬å–å‹é“¾é¡µé¢è¶…æ—¶ {url}")
            return []
        except requests.HTTPError as e:
            logger.error(f"çˆ¬å–å‹é“¾é¡µé¢HTTPé”™è¯¯ {url}: {e.response.status_code}")
            return []
        except Exception as e:
            logger.error(f"çˆ¬å–å‹é“¾é¡µé¢å¤±è´¥ {url}: {e}")
            return []


class RSSFetcher:
    """RSSæºè·å–å™¨"""
    
    def __init__(self, feed_suffixes: List[str], max_posts: int, cache_manager: Optional['CacheManager'] = None, 
                 request_timeout: int = 10, feed_check_timeout: int = 5, 
                 request_retries: int = 1, retry_backoff: float = 0.3, user_agent: str = None):
        self.feed_suffixes = feed_suffixes
        self.max_posts = max_posts
        self.cache = cache_manager
        self.request_timeout = request_timeout
        self.feed_check_timeout = feed_check_timeout
        self.request_retries = request_retries
        self.retry_backoff = retry_backoff
        self.user_agent = user_agent or DEFAULT_CONFIG['USER_AGENT']
        self.session = self._create_session()
        self.check_session = self._create_check_session()
        # æœ€è¿‘ä¸€æ¬¡è·å–/è§£æ RSS æ—¶çš„é”™è¯¯ä¿¡æ¯ï¼ˆå­—ç¬¦ä¸²ï¼‰ï¼Œä¾›å¤–éƒ¨æŸ¥è¯¢
        self.last_error: Optional[str] = None
    
    def _create_session(self) -> requests.Session:
        """åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„requestsä¼šè¯ï¼ˆç”¨äºè·å–RSSå†…å®¹ï¼‰"""
        session = requests.Session()
        
        retry_strategy = Retry(
            total=self.request_retries,
            backoff_factor=self.retry_backoff,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET", "HEAD"]
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        session.headers.update({'User-Agent': self.user_agent})
        session.verify = False
        return session
    
    def _create_check_session(self) -> requests.Session:
        """åˆ›å»ºä¸è¿›è¡Œé‡è¯•çš„ä¼šè¯ï¼ˆç”¨äºå¿«é€Ÿæ£€æŸ¥Feed URLï¼‰"""
        session = requests.Session()
        
        # ä¸è¿›è¡Œä»»ä½•é‡è¯•ï¼Œå¿«é€Ÿå¤±è´¥
        adapter = HTTPAdapter(max_retries=0)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        session.headers.update({'User-Agent': self.user_agent})
        session.verify = False
        return session
    
    def find_feed_url(self, base_url: str, custom_suffix: Optional[str] = None) -> Optional[str]:
        """å¯»æ‰¾ç«™ç‚¹çš„RSSæºURL
        
        ä¼˜å…ˆçº§ï¼š
        1. æ£€æŸ¥ç¼“å­˜
        2. å°è¯•è‡ªå®šä¹‰åç¼€ï¼ˆå¦‚æœæœ‰ï¼‰
        3. å°è¯•å¸¸è§Feedåç¼€ï¼ˆå¿«é€Ÿå¤±è´¥ï¼‰
        """
        # å…ˆæ£€æŸ¥ç¼“å­˜
        if self.cache:
            cached_url = self.cache.get_cached_feed_url(base_url)
            if cached_url:
                if self._check_feed_url(cached_url):
                    logger.debug(f"âœ“ ä½¿ç”¨ç¼“å­˜çš„Feed: {cached_url}")
                    return cached_url
        
        # ç¡®ä¿base_urlä»¥/ç»“å°¾
        base_url_normalized = base_url if base_url.endswith('/') else base_url + '/'
        
        # å¦‚æœæŒ‡å®šäº†è‡ªå®šä¹‰åç¼€ï¼Œé¦–å…ˆå°è¯•
        if custom_suffix:
            feed_url = urljoin(base_url_normalized, custom_suffix)
            if self._check_feed_url(feed_url):
                if self.cache:
                    self.cache.set_feed_url(base_url.rstrip('/'), feed_url)
                return feed_url
        
        # å°è¯•å¸¸è§çš„Feedåç¼€
        for suffix in self.feed_suffixes:
            feed_url = urljoin(base_url_normalized, suffix)
            if self._check_feed_url(feed_url):
                if self.cache:
                    self.cache.set_feed_url(base_url.rstrip('/'), feed_url)
                return feed_url
        
        return None
    
    def _check_feed_url(self, url: str) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦æ˜¯æœ‰æ•ˆçš„Feedæºï¼ˆå¿«é€Ÿæ£€æŸ¥ï¼Œä¸é‡è¯•ï¼‰"""
        try:
            # ä½¿ç”¨ä¸é‡è¯•çš„ä¼šè¯å’Œæ›´çŸ­çš„è¶…æ—¶
            response = self.check_session.get(url, timeout=self.feed_check_timeout)
            
            if response.status_code != 200:
                self.last_error = f"HTTP {response.status_code}"
                logger.debug(f"Feed URLæ£€æŸ¥å¤±è´¥ {url} (HTTP {response.status_code})")
                return False
            
            content_type = response.headers.get('content-type', '').lower()
            text_lower = response.text[:500].lower()  # åªæ£€æŸ¥å‰500å­—ç¬¦
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„XML/RSS/Atomæº
            is_feed = ('xml' in content_type or 'rss' in content_type or 'feed' in content_type or
                      '<?xml' in text_lower or '<rss' in text_lower or '<feed' in text_lower)
            
            if is_feed:
                logger.debug(f"âœ“ æ‰¾åˆ°æœ‰æ•ˆFeedæº: {url}")
            else:
                self.last_error = "not_feed_format"
                logger.debug(f"âœ— URLä¸æ˜¯Feedæ ¼å¼: {url}")

            return is_feed
                
        except requests.Timeout:
            logger.debug(f"Feed URLæ£€æŸ¥è¶…æ—¶: {url}")
            return False
        except requests.ConnectionError:
            logger.debug(f"Feed URLè¿æ¥å¤±è´¥: {url}")
            return False
        except Exception as e:
            logger.debug(f"Feed URLæ£€æŸ¥å¼‚å¸¸ {url}: {type(e).__name__}")
            return False
    
    def fetch_feed(self, feed_url: str) -> Optional[feedparser.FeedParserDict]:
        """è·å–å’Œè§£æRSSæº"""
        try:
            logger.info(f"æ­£åœ¨è·å–RSSæº: {feed_url}")
            
            # ä½¿ç”¨requestsè·å–å†…å®¹ï¼Œç„¶åä¼ ç»™feedparser
            response = self.session.get(feed_url, timeout=self.request_timeout)
            
            if response.status_code != 200:
                self.last_error = f"HTTP {response.status_code}"
                logger.warning(f"è·å–RSSæºå¤±è´¥ï¼ŒHTTP {response.status_code}: {feed_url}")
                return None
            
            feed = feedparser.parse(response.content)
            
            if feed.bozo and isinstance(feed.bozo_exception, Exception):
                self.last_error = str(feed.bozo_exception)
                logger.debug(f"RSSè§£æå¼‚å¸¸ {feed_url}: {feed.bozo_exception}")
            
            if not feed.entries:
                # æ— æ¡ç›®è§†ä¸ºè§£æ/å†…å®¹é—®é¢˜
                self.last_error = "empty_or_unparseable"
                logger.warning(f"RSSæºä¸ºç©ºæˆ–æ— æ³•è§£æ: {feed_url}")
                return None
            
            return feed
        except requests.Timeout:
            self.last_error = 'timeout'
            logger.warning(f"è·å–RSSæºè¶…æ—¶: {feed_url}")
            return None
        except requests.ConnectionError as e:
            self.last_error = type(e).__name__
            logger.warning(f"è·å–RSSæºè¿æ¥é”™è¯¯ {feed_url}: {type(e).__name__}")
            return None
        except requests.HTTPError as e:
            self.last_error = f"HTTPError {e.response.status_code}"
            logger.warning(f"è·å–RSSæºHTTPé”™è¯¯ {feed_url}: {e.response.status_code}")
            return None
        except Exception as e:
            self.last_error = str(e)
            logger.warning(f"è·å–RSSæºå¤±è´¥ {feed_url}: {type(e).__name__}")
            return None


class DataAggregator:
    """æ•°æ®èšåˆå™¨"""
    
    def __init__(self, max_posts: int, outdate_days: int, timezone_correction: bool = True, sort_by: str = 'pub_date'):
        self.max_posts = max_posts
        self.outdate_days = outdate_days
        self.timezone_correction = timezone_correction
        self.sort_by = sort_by  # 'pub_date' æˆ– 'updated_at'
        # å¦‚æœ outdate_days <= 0 åˆ™è¡¨ç¤ºä¸é™åˆ¶è¿‡æœŸï¼Œcutoff_time è®¾ä¸º None
        if outdate_days and outdate_days > 0:
            self.cutoff_time = get_beijing_time() - timedelta(days=outdate_days)
        else:
            self.cutoff_time = None
    
    def aggregate_feed(self, site_info: Dict[str, str], feed: feedparser.FeedParserDict) -> Dict[str, Any]:
        """èšåˆå•ä¸ªç«™ç‚¹çš„Feedæ•°æ®"""
        site_data = {
            'name': site_info['name'],
            'url': site_info['url'],
            'avatar': site_info['avatar'],
            'feed_url': site_info.get('feed_url', ''),
            'posts': []
        }
        
        # æå–Feedä¿¡æ¯
        feed_title = feed.feed.get('title', site_info['name'])
        
        posts = []
        for entry in feed.entries:
            try:
                # è·å–åŸå§‹æ—¶é—´å­—ç¬¦ä¸²
                published_str = getattr(entry, 'published', '')
                updated_str = getattr(entry, 'updated', '')

                # å¤„ç†å‘å¸ƒæ—¶é—´
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    pub_time = parse_feed_time(entry.published_parsed, self.timezone_correction, published_str or None)
                elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                    pub_time = parse_feed_time(entry.updated_parsed, self.timezone_correction, updated_str or None)
                else:
                    # æ²¡æœ‰è§£æåˆ°ä»»ä½•æ—¶é—´ï¼Œä½¿ç”¨å½“å‰åŒ—äº¬æ—¶é—´
                    pub_time = get_beijing_time()
                
                # è¿‡æ»¤è¿‡æœŸæ–‡ç« ï¼ˆå½“è®¾ç½®ä¸º0æˆ–è´Ÿæ•°æ—¶è¡¨ç¤ºä¸é™åˆ¶ï¼‰
                if self.cutoff_time is not None and pub_time < self.cutoff_time:
                    continue
                
                # å¤„ç†æ›´æ–°æ—¶é—´
                if hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                    update_time = parse_feed_time(entry.updated_parsed, self.timezone_correction, updated_str or None)
                else:
                    update_time = pub_time
                
                post = {
                    'title': entry.get('title', 'æ— æ ‡é¢˜'),
                    'link': entry.get('link', ''),
                    'description': entry.get('summary', ''),
                    'pub_date': pub_time.isoformat(),
                    'updated_at': update_time.isoformat(),
                    'author': entry.get('author', '')
                }
                posts.append(post)
            except Exception as e:
                logger.debug(f"å¤„ç†Feedæ¡ç›®å¤±è´¥: {e}")
                continue
        
        # æŒ‰é…ç½®çš„æ–¹å¼æ’åºå¹¶é™åˆ¶æ•°é‡
        posts.sort(key=lambda x: x[self.sort_by], reverse=True)
        site_data['posts'] = posts[:self.max_posts] if self.max_posts > 0 else posts
        
        return site_data
    
    def merge_data(self, all_sites: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆå¹¶æ‰€æœ‰ç«™ç‚¹æ•°æ®"""
        all_posts = []
        
        # æ”¶é›†æ‰€æœ‰æ–‡ç« 
        for site in all_sites:
            for post in site['posts']:
                post['site_name'] = site['name']
                post['site_url'] = site['url']
                post['avatar'] = site['avatar']
                all_posts.append(post)
        
        # æŒ‰é…ç½®çš„æ–¹å¼æ’åº
        all_posts.sort(key=lambda x: x[self.sort_by], reverse=True)
        
        return {
            'updated_at': get_beijing_time().isoformat(),
            'total_sites': len(all_sites),
            'total_posts': len(all_posts),
            'sites': all_sites,
            'all_posts': all_posts
        }


class FriendRSSAggregator:
    """ä¸»æ§åˆ¶å™¨"""
    
    def __init__(self, config_path: str = 'setting.yaml'):
        self.config = ConfigParser(config_path)
        self.cache = CacheManager(self.config.get_cache_file())
        self.site_filter = SiteFilter(
            self.config.get_block_sites(),
            self.config.get_block_site_reverse()
        )
        self.scraper = LinkPageScraper(
            self.config.get_link_page_rules(),
            self.config.get_request_timeout(),
            self.config.get_user_agent()
        )
        self.fetcher = RSSFetcher(
            self.config.get_feed_suffixes(),
            self.config.get_max_posts(),
            self.cache,
            self.config.get_request_timeout(),
            self.config.get_feed_check_timeout(),
            self.config.get_request_retries(),
            self.config.get_retry_backoff(),
            self.config.get_user_agent()
        )
        self.aggregator = DataAggregator(
            self.config.get_max_posts(),
            self.config.get_outdate_days(),
            self.config.get_timezone_correction(),
            self.config.get_sort_by()
        )
        # ç”¨äºè®°å½•è·å– RSS å¤±è´¥çš„ç«™ç‚¹åˆ—è¡¨
        self.failed_sites: List[Dict[str, Any]] = []
    
    def get_all_links(self) -> List[Dict[str, str]]:
        """è·å–æ‰€æœ‰å‹é“¾
        
        å¤„ç†é¡ºåºï¼š
        1. ä»å‹é“¾é¡µé¢çˆ¬å–é“¾æ¥
        2. å¯¹çˆ¬å–çš„é“¾æ¥è¿›è¡Œå±è”½æ£€æŸ¥
        3. å°è¯•è·å–RSSæºå¹¶ç¼“å­˜
        4. æ·»åŠ æ‰‹åŠ¨é…ç½®çš„é“¾æ¥
        """
        all_links = []
        url_set = set()
        
        # ã€ç¬¬ä¸€æ­¥ã€‘ä»å‹é“¾é¡µé¢çˆ¬å–é“¾æ¥ï¼Œå¹¶å°è¯•å‘ç°RSSæº
        logger.info("ã€ç¬¬ä¸€æ­¥ã€‘çˆ¬å–å‹é“¾é¡µé¢å¹¶å‘ç°RSSæº...")
        for page_url in self.config.get_link_pages():
            scraped_links = self.scraper.scrape(page_url)
            for link in scraped_links:
                # æ£€æŸ¥æ˜¯å¦è¢«å±è”½
                if self.site_filter.is_blocked(link['url']):
                    logger.debug(f"çˆ¬å–å‹é“¾è¢«å±è”½: {link['name']} ({link['url']})")
                    continue
                
                # å»é‡
                if link['url'] in url_set:
                    logger.debug(f"å‹é“¾å·²å­˜åœ¨ï¼Œè·³è¿‡é‡å¤: {link['name']}")
                    continue
                
                # å°è¯•å‘ç°RSSæº
                feed_url = self.fetcher.find_feed_url(link['url'])
                if feed_url:
                    link['feed_url'] = feed_url
                    logger.debug(f"å·²å‘ç°RSSæº: {link['name']} -> {feed_url}")
                else:
                    logger.debug(f"æœªæ‰¾åˆ°RSSæº: {link['name']}")
                
                all_links.append(link)
                url_set.add(link['url'])
                logger.debug(f"æ·»åŠ çˆ¬å–å‹é“¾: {link['name']} ({link['url']})")
        
        # ã€ç¬¬äºŒæ­¥ã€‘æ·»åŠ æ‰‹åŠ¨é…ç½®çš„é“¾æ¥
        logger.info("ã€ç¬¬äºŒæ­¥ã€‘æ·»åŠ æ‰‹åŠ¨é…ç½®çš„å‹é“¾...")
        manual_links = self.config.get_manual_links()
        for link in manual_links:
            # æ‰‹åŠ¨é…ç½®çš„å‹é“¾ä¸å—é»‘åå•é™åˆ¶ï¼Œä½†éœ€è¦æ£€æŸ¥å»é‡ï¼ˆå¿½ç•¥URLæœ«å°¾æ–œæ å·®å¼‚ï¼‰
            norm_url = link['url'].rstrip('/')
            existing_link = next((l for l in all_links if l.get('url', '').rstrip('/') == norm_url), None)

            if existing_link:
                # å·²å­˜åœ¨è¯¥ç«™ç‚¹ï¼šå¦‚æœæ‰‹åŠ¨é…ç½®æä¾›äº†è‡ªå®šä¹‰feed_suffixï¼Œåˆ™è¦†ç›–å·²æœ‰é…ç½®
                if link.get('feed_suffix'):
                    base = (existing_link['url'] if existing_link['url'].endswith('/') else existing_link['url'] + '/')
                    try:
                        feed_url = urljoin(base, link['feed_suffix'])
                        existing_link['feed_url'] = feed_url
                        existing_link['name'] = link['name'] or existing_link.get('name', '')
                        if link.get('avatar'):
                            existing_link['avatar'] = link['avatar']
                        logger.debug(f"æ‰‹åŠ¨å‹é“¾è¦†ç›–å·²å­˜åœ¨é¡¹: {link['name']} -> {feed_url}")
                    except Exception:
                        logger.debug(f"æ„å»ºè‡ªå®šä¹‰RSSæºå¤±è´¥: {link['name']} ({existing_link.get('url')})")
                else:
                    logger.debug(f"æ‰‹åŠ¨å‹é“¾å·²å­˜åœ¨ï¼Œè·³è¿‡é‡å¤: {link['name']}")
                continue
            
            # å¦‚æœæœ‰è‡ªå®šä¹‰Feedåç¼€ï¼ŒæŒ‰ç”¨æˆ·é€‰æ‹© Aï¼šè·³è¿‡å¿«é€Ÿæ£€æŸ¥ï¼Œç›´æ¥æ‹¼æ¥å¹¶è®¾ç½®ä¸º feed_urlï¼ˆfetch é˜¶æ®µä»ä¼šå°è¯•è§£æï¼‰
            if link.get('feed_suffix'):
                try:
                    base = link['url'] if link['url'].endswith('/') else link['url'] + '/'
                    feed_url = urljoin(base, link['feed_suffix'])
                    link['feed_url'] = feed_url
                    logger.debug(f"å·²è®¾ç½®è‡ªå®šä¹‰RSSæºï¼ˆè·³è¿‡æ£€æŸ¥ï¼‰: {link['name']} -> {feed_url}")
                except Exception:
                    logger.debug(f"æ„å»ºè‡ªå®šä¹‰RSSæºå¤±è´¥: {link['name']} ({link.get('url')})")
            else:
                feed_url = self.fetcher.find_feed_url(link['url'])
                if feed_url:
                    link['feed_url'] = feed_url
                    logger.debug(f"å·²å‘ç°RSSæº: {link['name']} -> {feed_url}")
            
            all_links.append(link)
            url_set.add(link['url'])
            logger.debug(f"æ·»åŠ æ‰‹åŠ¨å‹é“¾: {link['name']} ({link['url']})")
        
        logger.info(f"å…±è·å–{len(all_links)}æ¡æœ‰æ•ˆå‹é“¾")
        return all_links
    
    def process_site(self, link: Dict[str, str]) -> Optional[Dict[str, Any]]:
        """å¤„ç†å•ä¸ªç«™ç‚¹ï¼Œè·å–å…¶RSSæ•°æ®"""
        try:
            # å¦‚æœä¹‹å‰å·²ç»å‘ç°äº†Feed URLï¼Œç›´æ¥ä½¿ç”¨
            feed_url = link.get('feed_url')
            
            if not feed_url:
                # å¦‚æœæ²¡æœ‰é¢„å…ˆå‘ç°ï¼Œå†æ¬¡å°è¯•å¯»æ‰¾ï¼ˆå¤‡ç”¨ï¼‰
                feed_url = self.fetcher.find_feed_url(
                    link['url'],
                    link.get('feed_suffix')
                )
            
            if not feed_url:
                logger.warning(f"æ— æ³•æ‰¾åˆ°{link['name']}çš„RSSæº: {link['url']}")
                # è®°å½•å¤±è´¥ç«™ç‚¹
                self.failed_sites.append({
                    'name': link.get('name'),
                    'url': link.get('url'),
                    'feed_url': None,
                    'reason': 'no_feed_found'
                })
                return None
            
            # è·å–Feed
            feed = self.fetcher.fetch_feed(feed_url)
            if not feed:
                # è®°å½• fetch å¤±è´¥åŠå…¶åŸå› ï¼ˆfetcher.last_errorï¼‰
                self.failed_sites.append({
                    'name': link.get('name'),
                    'url': link.get('url'),
                    'feed_url': feed_url,
                    'reason': getattr(self.fetcher, 'last_error', 'fetch_failed')
                })
                return None
            
            site_info = {
                'name': link['name'],
                'url': link['url'],
                'avatar': link.get('avatar', ''),
                'feed_url': feed_url
            }
            
            site_data = self.aggregator.aggregate_feed(site_info, feed)
            logger.info(f"æˆåŠŸå¤„ç†{link['name']}: è·å–{len(site_data['posts'])}ç¯‡æ–‡ç« ")
            return site_data
        
        except Exception as e:
            logger.error(f"å¤„ç†ç«™ç‚¹{link.get('name', link['url'])}å¤±è´¥: {e}")
            self.failed_sites.append({
                'name': link.get('name'),
                'url': link.get('url'),
                'feed_url': link.get('feed_url'),
                'reason': str(e)
            })
            return None
    
    def run(self) -> dict:
        """æ‰§è¡Œä¸»æµç¨‹"""
        logger.info("=" * 50)
        logger.info("å¼€å§‹å‹é“¾RSSèšåˆ")
        logger.info("=" * 50)
        
        # è·å–æ‰€æœ‰é“¾æ¥
        all_links = self.get_all_links()
        
        # å¤„ç†æ¯ä¸ªç«™ç‚¹
        max_workers = self.config.get_max_workers()
        
        if max_workers and max_workers > 0:
            # å¹¶å‘å¤„ç†
            logger.info(f"ä½¿ç”¨{max_workers}ä¸ªçº¿ç¨‹å¹¶å‘å¤„ç†å‹é“¾...")
            all_sites = []
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_link = {executor.submit(self.process_site, link): link for link in all_links}
                
                # è·å–ç»“æœ
                for future in as_completed(future_to_link):
                    try:
                        site_data = future.result()
                        if site_data:
                            all_sites.append(site_data)
                    except Exception as e:
                        link = future_to_link[future]
                        logger.error(f"å¹¶å‘å¤„ç†ç«™ç‚¹{link.get('name')}æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
        else:
            # ä¸²è¡Œå¤„ç†
            all_sites = []
            for link in all_links:
                site_data = self.process_site(link)
                if site_data:
                    all_sites.append(site_data)
        
        # åˆå¹¶æ•°æ®
        final_data = self.aggregator.merge_data(all_sites)
        # æŠŠå¤±è´¥ç«™ç‚¹ä¿¡æ¯æ”¾å…¥æœ€ç»ˆç»“æœ
        final_data['failed_sites'] = self.failed_sites
        
        logger.info("=" * 50)
        logger.info(f"èšåˆå®Œæˆ: {final_data['total_sites']}ä¸ªç«™ç‚¹, {final_data['total_posts']}ç¯‡æ–‡ç« ")
        logger.info("=" * 50)
        
        # ä¿å­˜ç¼“å­˜
        self.cache.save()
        
        return final_data
    
    def save_to_file(self, data: dict, output_path: str = 'data.json'):
        """ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            logger.info(f"æ•°æ®å·²ä¿å­˜åˆ°{output_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    try:
        aggregator = FriendRSSAggregator('setting.yaml')
        data = aggregator.run()
        # ä»é…ç½®ä¸­è¯»å–è¾“å‡ºæ–‡ä»¶å
        output_name = aggregator.config.get_output_filename()
        aggregator.save_to_file(data, output_name)
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        logger.info("ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        logger.info(f"  âœ“ æ€»ç«™ç‚¹æ•°: {data['total_sites']}")
        logger.info(f"  âœ“ æ€»æ–‡ç« æ•°: {data['total_posts']}")
        logger.info(f"  âœ“ æ›´æ–°æ—¶é—´: {data['updated_at']}")
        logger.info("âœ… ç¨‹åºæ‰§è¡ŒæˆåŠŸ!")
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        raise


if __name__ == '__main__':
    main()
